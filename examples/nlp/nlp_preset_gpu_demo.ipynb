{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import usual libraries\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = 'gpu'\n",
    "\n",
    "import cudf\n",
    "\n",
    "# import lightautoml_gpu\n",
    "from lightautoml_gpu.automl.presets.text_presets import TabularNLPAutoML\n",
    "from lightautoml_gpu.automl.presets.gpu.text_gpu_presets import TabularNLPAutoMLGPU\n",
    "from lightautoml_gpu.tasks import Task\n",
    "from lightautoml_gpu.dataset.utils import roles_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define nlp constants\n",
    "N_THREADS = 4\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "TIMEOUT = 300\n",
    "TARGET_NAME = 'is_good'\n",
    "\n",
    "torch.set_num_threads(N_THREADS)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bankiru dataset\n",
    "DATASET_FULLNAME = '../../data/nlp/bankiru_isgood.csv'\n",
    "\n",
    "# here only 1000 samples are used for time reasons (for a detailed check, one needs to use larger number:\n",
    "# 100k-500k)\n",
    "data = pd.read_csv(DATASET_FULLNAME)[[\"message\", \"title\", \"is_good\"]].fillna(\"\")[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message  \\\n",
      "0                                      Здравствуйте.   \n",
      "1  https://ibb.co/qdvV4Fyhttps://ibb.co/Pzqxd2vht...   \n",
      "2  Добрый день! Сегодня я обращалась на горячую л...   \n",
      "3  31 марта 2021 года заключил договор рефинансир...   \n",
      "4  Заметил, что с моей кредитной карты банка Тинь...   \n",
      "\n",
      "                                               title  is_good  \n",
      "0                                   Создание шаблона        0  \n",
      "1                                             Кизляр        1  \n",
      "2                               Хорошее обслуживание        1  \n",
      "3  Пинают меня в страховую, страховая обратно в банк        0  \n",
      "4               Списание средств сторонними услугами        1  \n",
      "Data splitted. Parts sizes: tr_data = (800, 3), te_data = (200, 3)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "tr_data, te_data = train_test_split(data,\n",
    "        test_size=TEST_SIZE,\n",
    "        stratify=data[TARGET_NAME],\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "print(data.head())\n",
    "tr_data = pd.DataFrame(data, index=[i for i in range(tr_data.shape[0])])\n",
    "te_data = pd.DataFrame(data, index=[i for i in range(te_data.shape[0])])\n",
    "\n",
    "print(f'Data splitted. Parts sizes: tr_data = {tr_data.shape}, te_data = {te_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'text', 'title': 'text', 'is_good': 'target'}\n"
     ]
    }
   ],
   "source": [
    "# define task and roles\n",
    "task = Task('binary', device=device)\n",
    "\n",
    "roles = {\n",
    "    'text': ['message', 'title'],\n",
    "    'target': TARGET_NAME,\n",
    "}\n",
    "print(roles_parser(roles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_automl(automl, tr_data, te_data):\n",
    "    t0 = time.time()\n",
    "    oof_pred = automl.fit_predict(tr_data, roles=roles, verbose=1)\n",
    "    t1 = time.time()\n",
    "    print('Elapsed time (train): {}'.format(t1 - t0))\n",
    "\n",
    "    t0 = time.time()\n",
    "    te_pred = automl.predict(te_data)\n",
    "    t1 = time.time()\n",
    "    print('Elapsed time (test): {}'.format(t1 - t0))\n",
    "\n",
    "    not_nan = np.any(~np.isnan(oof_pred.data), axis=1)\n",
    "    print(f'OOF score: {roc_auc_score(tr_data[TARGET_NAME].values[not_nan], oof_pred.data[not_nan][:, 0])}')\n",
    "    print(f'TEST score: {roc_auc_score(te_data[TARGET_NAME].values, te_pred.data[:, 0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear_l2 model with different text features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidf text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 100\n",
    "n_oversample = 0\n",
    "ngram = (1, 1)\n",
    "\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "            timeout=600,\n",
    "            cpu_limit=1,\n",
    "            gpu_ids='0',\n",
    "            client=None,\n",
    "            general_params={\n",
    "                'nested_cv': False,\n",
    "                'use_algos': [['linear_l2']]\n",
    "            },\n",
    "            reader_params={\n",
    "                'npartitions': 2\n",
    "            },\n",
    "            text_params={\n",
    "                'lang': 'ru',\n",
    "                'verbose': False,\n",
    "                'use_stem': False,\n",
    "            },\n",
    "            tfidf_params={\n",
    "                'n_components': n_components,\n",
    "                'n_oversample': n_oversample,\n",
    "                'tfidf_params': {'ngram_range': ngram}\n",
    "            },\n",
    "            linear_pipeline_params={\n",
    "                'text_features': \"tfidf\"\n",
    "            }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:28:40] Stdout logging level is INFO.\n",
      "[19:28:40] Task: binary\n",
      "\n",
      "[19:28:40] Start automl preset with listed constraints:\n",
      "[19:28:40] - time: 600.00 seconds\n",
      "[19:28:40] - CPU: 1 cores\n",
      "[19:28:40] - memory: 16 GB\n",
      "\n",
      "[19:28:40] Train data shape: (800, 3)\n",
      "[19:28:40] Layer \u001b[1m1\u001b[0m train process start. Time left 599.96 secs\n",
      "[19:28:43] Start fitting Lvl_0_Pipe_0_Mod_0_LinearL2 ...\n",
      "[19:28:43] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "[19:28:43] Linear model: C = 1e-05 score = 0.8947904706001282\n",
      "[19:28:43] Linear model: C = 5e-05 score = 0.887622058391571\n",
      "[19:28:43] Linear model: C = 0.0001 score = 0.8875912427902222\n",
      "[19:28:43] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "[19:28:43] Linear model: C = 1e-05 score = 0.8541589379310608\n",
      "[19:28:43] Linear model: C = 5e-05 score = 0.8557347059249878\n",
      "[19:28:43] Linear model: C = 0.0001 score = 0.8557347059249878\n",
      "[19:28:44] Linear model: C = 0.0005 score = 0.8557348251342773\n",
      "[19:28:44] Linear model: C = 0.001 score = 0.8557348251342773\n",
      "[19:28:44] Linear model: C = 0.005 score = 0.8561055064201355\n",
      "[19:28:44] Linear model: C = 0.01 score = 0.8565999269485474\n",
      "[19:28:44] Linear model: C = 0.05 score = 0.8583920001983643\n",
      "[19:28:44] Linear model: C = 0.1 score = 0.8621615767478943\n",
      "[19:28:44] Linear model: C = 0.5 score = 0.8727289438247681\n",
      "[19:28:44] Linear model: C = 1 score = 0.8765603303909302\n",
      "[19:28:44] Linear model: C = 5 score = 0.8768075704574585\n",
      "[19:28:44] Linear model: C = 10 score = 0.8726671934127808\n",
      "[19:28:44] Linear model: C = 50 score = 0.8590100407600403\n",
      "[19:28:44] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "[19:28:44] Linear model: C = 1e-05 score = 0.877183198928833\n",
      "[19:28:44] Linear model: C = 5e-05 score = 0.877214252948761\n",
      "[19:28:44] Linear model: C = 0.0001 score = 0.8772763609886169\n",
      "[19:28:44] Linear model: C = 0.0005 score = 0.8773696422576904\n",
      "[19:28:44] Linear model: C = 0.001 score = 0.8775560855865479\n",
      "[19:28:44] Linear model: C = 0.005 score = 0.8787369728088379\n",
      "[19:28:44] Linear model: C = 0.01 score = 0.8797314763069153\n",
      "[19:28:44] Linear model: C = 0.05 score = 0.885263204574585\n",
      "[19:28:44] Linear model: C = 0.1 score = 0.8884952068328857\n",
      "[19:28:44] Linear model: C = 0.5 score = 0.9001802206039429\n",
      "[19:28:45] Linear model: C = 1 score = 0.9025420546531677\n",
      "[19:28:45] Linear model: C = 5 score = 0.8998695611953735\n",
      "[19:28:45] Linear model: C = 10 score = 0.8963266611099243\n",
      "[19:28:45] Lvl_0_Pipe_0_Mod_0_LinearL2 fitting and predicting completed\n",
      "[19:28:45] Time left 595.48 secs\n",
      "\n",
      "[19:28:45] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[19:28:45] \u001b[1mAutoml preset training completed in 4.52 seconds\u001b[0m\n",
      "\n",
      "[19:28:45] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (3 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) \n",
      "\n",
      "Elapsed time (train): 4.525374889373779\n",
      "Elapsed time (test): 0.8524894714355469\n",
      "OOF score: 0.8394423461911542\n",
      "TEST score: 0.9575827055355387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "814"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### tfidf_subword features\n",
    "\n",
    "The following __text_params__ work only with __tfidf_subword__ text features:   \n",
    "__vocab_path__ - path to vocabulary .txt file,  \n",
    "__data_path__ - .txt file (saved pd.Series) for the tokenizer to be trained on (if vocab is not specified)  \n",
    "__is_hash__ - True means vocab is not raw vocab but was transformed with hash_vocab function from cudf,  \n",
    "__max_length__ - max number of tokens to leave in one text (exceeding ones would be truncated)  \n",
    "__tokenizer__ - [\"bpe\" or \"wordpiece\"] if vocab is None. Type of tokenizer to be trained  \n",
    "__vocab_size__ - vocabulary size for trained tokenizer  \n",
    "__save_path__ - path where trained vocabulary would be saved to  \n",
    "\n",
    "Overall, there are 3 possible scenarios to run tfidf_subword text features:  \n",
    "1) __vocab_path__ is defined, __is_hash__ = True. It means that __vocab_path__ contains path to a hashed version of vocabulary. No additional transformation is needed. This is the optimal usage (all vocabulary pre-processing was done in advance).\n",
    "2) __vocab_path__ is defined, __is_hash__ = False. __vocab_path__ contains path to a vocabulary with raw words, it needs to be transformed to a hash version. This is the second fastest option.\n",
    "3) __vocab_path__ is not defined, __data_path__ is defined (with additional parameters __tokenizer__, __vocab_size__ and __save_path__). Only .txt file of a dataframe is available. Note, that it works not with a dataframe itself but with its .txt version. One should be careful with tokenizer settings. Recommended way is to study the dataset in advance, tweak tokenizer settings and create the vocabulary aside from LAMA pipeline. The quality of __tfidf_subword__ text features highly depend on the quality of the used tokenizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for all scenarios. Imagine that only pd.Series of text data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: how to create .txt dataframe (one should save only text corpus)\n",
    "# This is an example, it is not necessary to run it\n",
    "\n",
    "# Step 1. Choose your representative text data and save it to .txt file. Here only one column of text dataset \n",
    "# is taken but sometimes it might be a good idea to concatenate all text columns instead of choosing one.\n",
    "data_text = data['message']\n",
    "file_data_text = 'bankiru_isgood_test.txt'\n",
    "with codecs.open(file_data_text, 'w+', 'utf-8') as f:\n",
    "    for i in range(len(data_text)):\n",
    "        f.write(data_text.iloc[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: how to use huggingface tokenizer to create vocabulary from .txt dataframe\n",
    "# This is an example, it is not necessary to run it\n",
    "\n",
    "# Step 2. Having a text data file, train token vocabulary.\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n",
    "\n",
    "tokenizer = 'bpe' # or 'wordpiece'\n",
    "vocab_size = 30000\n",
    "data_path = file_data_text # path to a .txt pd.Series of text data\n",
    "vocab_save_path = f\"{tokenizer}_{vocab_size // 1000}k_test.txt\"\n",
    "\n",
    "if tokenizer == \"bpe\":\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    trainer = BpeTrainer(\n",
    "        vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[SEP]\", \"[CLS]\"]\n",
    "    )\n",
    "else:\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    trainer = WordPieceTrainer(\n",
    "        vocab_size=vocab_size, special_tokens=[\"[UNK]\", \"[SEP]\", \"[CLS]\"]\n",
    "    )\n",
    "tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "tokenizer.train([data_path], trainer) # train tokenizer on out .txt text data\n",
    "trained_vocab = tokenizer.get_vocab()\n",
    "\n",
    "# save trained vocabulary to a .txt file\n",
    "with codecs.open(vocab_save_path, 'w+', 'utf-8') as f:\n",
    "    for key in trained_vocab.keys():\n",
    "        f.write(key + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to build table using 1.499947n space\n",
      "Longest bin was 12\n",
      "Processing bin 0 / 4702 of size = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishat/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cudf/utils/hash_vocab_utils.py:49: RuntimeWarning: overflow encountered in ulong_scalars\n",
      "  return ((a * k + b) % PRIME) % size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bin 500 / 4702 of size = 3\n",
      "Processing bin 1000 / 4702 of size = 3\n",
      "Processing bin 1500 / 4702 of size = 3\n",
      "Processing bin 2000 / 4702 of size = 3\n",
      "Processing bin 2500 / 4702 of size = 6\n",
      "Processing bin 3000 / 4702 of size = 4\n",
      "Processing bin 3500 / 4702 of size = 6\n",
      "Processing bin 4000 / 4702 of size = 2\n",
      "Processing bin 4500 / 4702 of size = 2\n",
      "Final table size 18810 elements compared to 18810 for original\n",
      "Max bin length was 12\n",
      "All present tokens return correct value.\n"
     ]
    }
   ],
   "source": [
    "# Note: how to create hash vocabulary from word .txt vocabulary\n",
    "# This is an example, it is not necessary to run it\n",
    "\n",
    "# Step 3. Having .txt vocabulary file, create a hashed version of it which would be used by \n",
    "# cudf.SubwordTokenizer\n",
    "from cudf.utils.hash_vocab_utils import hash_vocab\n",
    "\n",
    "vocab_save_path_hash = vocab_save_path.split('.')[0]+'_hash.txt'\n",
    "hash_vocab(vocab_save_path, vocab_save_path_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-21 19:28:53--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.132.77, 52.216.136.166, 52.216.140.22, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.132.77|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 231508 (226K) [text/plain]\n",
      "Saving to: ‘bert-base-uncased-vocab.txt’\n",
      "\n",
      "bert-base-uncased-v 100%[===================>] 226,08K   506KB/s    in 0,4s    \n",
      "\n",
      "2022-11-21 19:28:55 (506 KB/s) - ‘bert-base-uncased-vocab.txt’ saved [231508/231508]\n",
      "\n",
      "--2022-11-21 19:28:55--  https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased/vocab.txt\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.140.22, 54.231.165.104, 52.216.136.166, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.140.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1649718 (1,6M) [text/plain]\n",
      "Saving to: ‘vocab.txt’\n",
      "\n",
      "vocab.txt           100%[===================>]   1,57M   366KB/s    in 4,4s    \n",
      "\n",
      "2022-11-21 19:29:00 (366 KB/s) - ‘vocab.txt’ saved [1649718/1649718]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternative Step 1-2. Download existing vocabulary (one could use data from huggingfsce models).\n",
    "\n",
    "# Download standard bert English vocabulary\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
    "bert_vocab_en_path = 'bert-base-uncased-vocab.txt'\n",
    "# Download bert Russian vocabulary\n",
    "!wget https://s3.amazonaws.com/models.huggingface.co/bert/DeepPavlov/rubert-base-cased/vocab.txt\n",
    "bert_vocab_ru_path = 'vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True to use data generated in this notebook, False to use data available in zip dataset archive\n",
    "use_test_data = True \n",
    "\n",
    "if use_test_data:\n",
    "    bankiru_info = {'path': '../../data/nlp/bankiru_isgood.csv',\n",
    "                    'text_roles': ['message', 'title'],\n",
    "                    'target': 'is_good',\n",
    "                    'task': 'binary',\n",
    "                    'lang': 'ru',\n",
    "                    'csv2text': file_data_text,\n",
    "                    'vocab_path': vocab_save_path,\n",
    "                    'vocab_hash_path': vocab_save_path_hash\n",
    "    }\n",
    "else:\n",
    "    bankiru_info = {'path': '../../data/nlp/bankiru_isgood.csv',\n",
    "                    'text_roles': ['message', 'title'],\n",
    "                    'target': 'is_good',\n",
    "                    'task': 'binary',\n",
    "                    'lang': 'ru',\n",
    "                    'csv2text': '../../data/nlp/csv2text/bankiru_isgood.txt',\n",
    "                    'vocab_path': '../../data/nlp/vocab/bankiru_isgood_vocab.txt',\n",
    "                    'vocab_hash_path': '../../data/nlp/vocab_hash/bankiru_isgood_vocab_hash.txt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario 1\n",
    "automl = TabularNLPAutoMLGPU(task=task, \n",
    "                              timeout=600, \n",
    "                              cpu_limit=1, \n",
    "                              gpu_ids='0', \n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False,\n",
    "                                  'vocab_path': bankiru_info['vocab_hash_path'],\n",
    "                                  'is_hash': True,\n",
    "                                  # 'data_path': file_name,\n",
    "                                  # 'tokenizer': \"bpe\",\n",
    "                                  # 'vocab_size': 30000\n",
    "                              },\n",
    "                              tfidf_params={\n",
    "                                  'n_components': n_components,\n",
    "                                  'n_oversample': n_oversample,\n",
    "                                  'tfidf_params': {'ngram_range': ngram}\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'tfidf_subword'\n",
    "                              },\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:29:00] Stdout logging level is INFO.\n",
      "[19:29:00] Task: binary\n",
      "\n",
      "[19:29:00] Start automl preset with listed constraints:\n",
      "[19:29:00] - time: 600.00 seconds\n",
      "[19:29:00] - CPU: 1 cores\n",
      "[19:29:00] - memory: 16 GB\n",
      "\n",
      "[19:29:00] Train data shape: (800, 3)\n",
      "[19:29:00] Layer \u001b[1m1\u001b[0m train process start. Time left 599.98 secs\n",
      "[19:29:02] Start fitting Lvl_0_Pipe_0_Mod_0_LinearL2 ...\n",
      "[19:29:02] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "[19:29:02] Linear model: C = 1e-05 score = 0.8910208940505981\n",
      "[19:29:02] Linear model: C = 5e-05 score = 0.8903102874755859\n",
      "[19:29:02] Linear model: C = 0.0001 score = 0.8903720378875732\n",
      "[19:29:02] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "[19:29:02] Linear model: C = 1e-05 score = 0.8680941462516785\n",
      "[19:29:02] Linear model: C = 5e-05 score = 0.8635520935058594\n",
      "[19:29:02] Linear model: C = 0.0001 score = 0.8636138439178467\n",
      "[19:29:02] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "[19:29:02] Linear model: C = 1e-05 score = 0.7680402398109436\n",
      "[19:29:02] Linear model: C = 5e-05 score = 0.7659581303596497\n",
      "[19:29:02] Linear model: C = 0.0001 score = 0.7660202383995056\n",
      "[19:29:02] Lvl_0_Pipe_0_Mod_0_LinearL2 fitting and predicting completed\n",
      "[19:29:02] Time left 598.10 secs\n",
      "\n",
      "[19:29:02] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[19:29:02] \u001b[1mAutoml preset training completed in 1.90 seconds\u001b[0m\n",
      "\n",
      "[19:29:02] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (3 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) \n",
      "\n",
      "Elapsed time (train): 1.9124763011932373\n",
      "Elapsed time (test): 0.9560441970825195\n",
      "OOF score: 0.6519582550788048\n",
      "TEST score: 0.8437056447210394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario 2\n",
    "automl = TabularNLPAutoMLGPU(task=task, \n",
    "                              timeout=600, \n",
    "                              cpu_limit=1, \n",
    "                              gpu_ids='0', \n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False,\n",
    "                                  'vocab_path': bankiru_info['vocab_path'],\n",
    "                                  'is_hash': False,\n",
    "                                  # 'data_path': file_name,\n",
    "                                  # 'tokenizer': \"bpe\",\n",
    "                                  # 'vocab_size': 30000\n",
    "                              },\n",
    "                              tfidf_params={\n",
    "                                  'n_components': n_components,\n",
    "                                  'n_oversample': n_oversample,\n",
    "                                  'tfidf_params': {'ngram_range': ngram}\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'tfidf_subword'\n",
    "                              },\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:29:03] Stdout logging level is INFO.\n",
      "[19:29:03] Task: binary\n",
      "\n",
      "[19:29:03] Start automl preset with listed constraints:\n",
      "[19:29:03] - time: 600.00 seconds\n",
      "[19:29:03] - CPU: 1 cores\n",
      "[19:29:03] - memory: 16 GB\n",
      "\n",
      "[19:29:03] Train data shape: (800, 3)\n",
      "[19:29:03] Layer \u001b[1m1\u001b[0m train process start. Time left 599.98 secs\n",
      "Attempting to build table using 1.499947n space\n",
      "Longest bin was 12\n",
      "Processing bin 0 / 4702 of size = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishat/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cudf/utils/hash_vocab_utils.py:49: RuntimeWarning: overflow encountered in ulong_scalars\n",
      "  return ((a * k + b) % PRIME) % size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bin 500 / 4702 of size = 3\n",
      "Processing bin 1000 / 4702 of size = 3\n",
      "Processing bin 1500 / 4702 of size = 3\n",
      "Processing bin 2000 / 4702 of size = 3\n",
      "Processing bin 2500 / 4702 of size = 6\n",
      "Processing bin 3000 / 4702 of size = 4\n",
      "Processing bin 3500 / 4702 of size = 6\n",
      "Processing bin 4000 / 4702 of size = 2\n",
      "Processing bin 4500 / 4702 of size = 2\n",
      "Final table size 18810 elements compared to 18810 for original\n",
      "Max bin length was 12\n",
      "All present tokens return correct value.\n",
      "[19:29:12] Start fitting Lvl_0_Pipe_0_Mod_0_LinearL2 ...\n",
      "[19:29:12] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "[19:29:12] Linear model: C = 1e-05 score = 0.8885180354118347\n",
      "[19:29:12] Linear model: C = 5e-05 score = 0.8906809687614441\n",
      "[19:29:12] Linear model: C = 0.0001 score = 0.8904646635055542\n",
      "[19:29:12] Linear model: C = 0.0005 score = 0.8903411030769348\n",
      "[19:29:12] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "[19:29:12] Linear model: C = 1e-05 score = 0.8656840324401855\n",
      "[19:29:12] Linear model: C = 5e-05 score = 0.8641083240509033\n",
      "[19:29:12] Linear model: C = 0.0001 score = 0.8641083240509033\n",
      "[19:29:12] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "[19:29:12] Linear model: C = 1e-05 score = 0.7659891843795776\n",
      "[19:29:12] Linear model: C = 5e-05 score = 0.7659892439842224\n",
      "[19:29:12] Linear model: C = 0.0001 score = 0.7660202383995056\n",
      "[19:29:12] Linear model: C = 0.0005 score = 0.7661756277084351\n",
      "[19:29:12] Linear model: C = 0.001 score = 0.766486406326294\n",
      "[19:29:12] Linear model: C = 0.005 score = 0.7682889103889465\n",
      "[19:29:13] Linear model: C = 0.01 score = 0.7705264687538147\n",
      "[19:29:13] Linear model: C = 0.05 score = 0.7846976518630981\n",
      "[19:29:13] Linear model: C = 0.1 score = 0.7968177199363708\n",
      "[19:29:13] Linear model: C = 0.5 score = 0.8375908732414246\n",
      "[19:29:13] Linear model: C = 1 score = 0.8562993407249451\n",
      "[19:29:13] Linear model: C = 5 score = 0.8749456405639648\n",
      "[19:29:13] Linear model: C = 10 score = 0.8761886358261108\n",
      "[19:29:13] Linear model: C = 50 score = 0.8732674717903137\n",
      "[19:29:13] Linear model: C = 100 score = 0.8732674717903137\n",
      "[19:29:13] Lvl_0_Pipe_0_Mod_0_LinearL2 fitting and predicting completed\n",
      "[19:29:13] Time left 590.22 secs\n",
      "\n",
      "[19:29:13] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[19:29:13] \u001b[1mAutoml preset training completed in 9.78 seconds\u001b[0m\n",
      "\n",
      "[19:29:13] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (3 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) \n",
      "\n",
      "Elapsed time (train): 9.788661241531372\n",
      "Elapsed time (test): 1.0149738788604736\n",
      "OOF score: 0.8308773450560337\n",
      "TEST score: 0.9473195763729665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "788"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario 3\n",
    "automl = TabularNLPAutoMLGPU(task=task, \n",
    "                              timeout=600, \n",
    "                              cpu_limit=1, \n",
    "                              gpu_ids='0', \n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False,\n",
    "                                  'vocab_path': None,\n",
    "                                  'data_path': bankiru_info['csv2text'],\n",
    "                                  'tokenizer': \"bpe\",\n",
    "                                  'vocab_size': 30000\n",
    "                              },\n",
    "                              tfidf_params={\n",
    "                                  'n_components': n_components,\n",
    "                                  'n_oversample': n_oversample,\n",
    "                                  'tfidf_params': {'ngram_range': ngram}\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'tfidf_subword'\n",
    "                              },\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:29:21] Stdout logging level is INFO.\n",
      "[19:29:21] Task: binary\n",
      "\n",
      "[19:29:21] Start automl preset with listed constraints:\n",
      "[19:29:21] - time: 600.00 seconds\n",
      "[19:29:21] - CPU: 1 cores\n",
      "[19:29:21] - memory: 16 GB\n",
      "\n",
      "[19:29:21] Train data shape: (800, 3)\n",
      "[19:29:21] Layer \u001b[1m1\u001b[0m train process start. Time left 599.98 secs\n",
      "\n",
      "\n",
      "\n",
      "Attempting to build table using 1.499947n space\n",
      "Longest bin was 12\n",
      "Processing bin 0 / 4702 of size = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishat/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cudf/utils/hash_vocab_utils.py:49: RuntimeWarning: overflow encountered in ulong_scalars\n",
      "  return ((a * k + b) % PRIME) % size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bin 500 / 4702 of size = 3\n",
      "Processing bin 1000 / 4702 of size = 3\n",
      "Processing bin 1500 / 4702 of size = 3\n",
      "Processing bin 2000 / 4702 of size = 3\n",
      "Processing bin 2500 / 4702 of size = 6\n",
      "Processing bin 3000 / 4702 of size = 4\n",
      "Processing bin 3500 / 4702 of size = 6\n",
      "Processing bin 4000 / 4702 of size = 2\n",
      "Processing bin 4500 / 4702 of size = 2\n",
      "Final table size 18810 elements compared to 18810 for original\n",
      "Max bin length was 12\n",
      "All present tokens return correct value.\n",
      "[19:29:30] Start fitting Lvl_0_Pipe_0_Mod_0_LinearL2 ...\n",
      "[19:29:30] ===== Start working with \u001b[1mfold 0\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "[19:29:31] Linear model: C = 1e-05 score = 0.8909590840339661\n",
      "[19:29:31] Linear model: C = 5e-05 score = 0.8903101682662964\n",
      "[19:29:31] Linear model: C = 0.0001 score = 0.8903102874755859\n",
      "[19:29:31] ===== Start working with \u001b[1mfold 1\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "[19:29:31] Linear model: C = 1e-05 score = 0.8661784529685974\n",
      "[19:29:31] Linear model: C = 5e-05 score = 0.8640155792236328\n",
      "[19:29:31] Linear model: C = 0.0001 score = 0.8641082644462585\n",
      "[19:29:31] ===== Start working with \u001b[1mfold 2\u001b[0m for \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m (orig) =====\n",
      "[19:29:31] Linear model: C = 1e-05 score = 0.7672633528709412\n",
      "[19:29:31] Linear model: C = 5e-05 score = 0.7659580707550049\n",
      "[19:29:31] Linear model: C = 0.0001 score = 0.7659581303596497\n",
      "[19:29:31] Lvl_0_Pipe_0_Mod_0_LinearL2 fitting and predicting completed\n",
      "[19:29:31] Time left 590.35 secs\n",
      "\n",
      "[19:29:31] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[19:29:31] \u001b[1mAutoml preset training completed in 9.65 seconds\u001b[0m\n",
      "\n",
      "[19:29:31] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 1.00000 * (3 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) \n",
      "\n",
      "Elapsed time (train): 9.661709308624268\n",
      "Elapsed time (test): 0.9738724231719971\n",
      "OOF score: 0.6983950082210252\n",
      "TEST score: 0.8437056447210395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1583"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### embed text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One should note that gensim package was removed, now only torchnlp embeddings are available of fixed\n",
    "# dimensionality\n",
    "model_name = 'random_lstm'\n",
    "\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "                              timeout=600,\n",
    "                              cpu_limit=1,\n",
    "                              gpu_ids='0',\n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False\n",
    "                              },\n",
    "                              autonlp_params={\n",
    "                                  'model_name': model_name,\n",
    "                                  'sent_scaler': 'l1',\n",
    "                                  'embedding_model': 'fasttext', # now this has a different meaning\n",
    "                                  'cache_dir': None\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'embed'\n",
    "                              },\n",
    "                              )\n",
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'borep'\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "                              timeout=600,\n",
    "                              cpu_limit=1,\n",
    "                              gpu_ids='0',\n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False\n",
    "                              },\n",
    "                              autonlp_params={\n",
    "                                  'model_name': model_name,\n",
    "                                  'sent_scaler': None,\n",
    "                                  'embedding_model': 'fasttext', # now this has a different meaning\n",
    "                                  'cache_dir': None\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'embed'\n",
    "                              },\n",
    "                              )\n",
    "\n",
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'random_lstm_bert'\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "                              timeout=600,\n",
    "                              cpu_limit=1,\n",
    "                              gpu_ids='0',\n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False\n",
    "                              },\n",
    "                              autonlp_params={\n",
    "                                  'model_name': model_name,\n",
    "                                  'sent_scaler': 'l2',\n",
    "                                  'embedding_model': 'fasttext', # now this has a different meaning\n",
    "                                  'cache_dir': None\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'embed'\n",
    "                              },\n",
    "                              )\n",
    "\n",
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pooled_bert'\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "                              timeout=600,\n",
    "                              cpu_limit=1,\n",
    "                              gpu_ids='0',\n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False\n",
    "                              },\n",
    "                              autonlp_params={\n",
    "                                  'model_name': model_name,\n",
    "                                  'sent_scaler': 'l2',\n",
    "                                  'embedding_model': 'fasttext', # now this has a different meaning\n",
    "                                  'cache_dir': None\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'embed'\n",
    "                              },\n",
    "                              )\n",
    "\n",
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'wat'\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "                              timeout=600,\n",
    "                              cpu_limit=1,\n",
    "                              gpu_ids='0',\n",
    "                              client=None,\n",
    "                              general_params={\n",
    "                                  'nested_cv': False,\n",
    "                                  'use_algos': [['linear_l2']]\n",
    "                              },\n",
    "                              reader_params={\n",
    "                                  'npartitions': 2\n",
    "                              },\n",
    "                              text_params={\n",
    "                                  'lang': 'ru',\n",
    "                                  'verbose': False,\n",
    "                                  'use_stem': False\n",
    "                              },\n",
    "                              autonlp_params={\n",
    "                                  'model_name': model_name,\n",
    "                                  'sent_scaler': None,\n",
    "                                  'embedding_model': 'fasttext', # now this has a different meaning\n",
    "                                  'cache_dir': None\n",
    "                              },\n",
    "                              linear_pipeline_params={\n",
    "                                  'text_features': 'embed'\n",
    "                              },\n",
    "                              )\n",
    "\n",
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catboost and xgb algos with tfidf text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost\n",
    "n_components = 100\n",
    "n_oversample = 0\n",
    "ngram = (1, 1)\n",
    "\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "            timeout=600,\n",
    "            cpu_limit=1,\n",
    "            gpu_ids='0',\n",
    "            client=None,\n",
    "            general_params={\n",
    "                'nested_cv': False,\n",
    "                'use_algos': [['cb']]\n",
    "            },\n",
    "            reader_params={\n",
    "                'npartitions': 2\n",
    "            },\n",
    "            text_params={\n",
    "                'lang': 'ru',\n",
    "                'verbose': False,\n",
    "                'use_stem': False,\n",
    "            },\n",
    "            tfidf_params={\n",
    "                'n_components': n_components,\n",
    "                'n_oversample': n_oversample,\n",
    "                'tfidf_params': {'ngram_range': ngram}\n",
    "            },\n",
    "            linear_pipeline_params={\n",
    "                'text_features': \"tfidf\"\n",
    "            }\n",
    "            )\n",
    "\n",
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "n_components = 100\n",
    "n_oversample = 0\n",
    "ngram = (1, 1)\n",
    "\n",
    "automl = TabularNLPAutoMLGPU(task=task,\n",
    "            timeout=600,\n",
    "            cpu_limit=1,\n",
    "            gpu_ids='0',\n",
    "            client=None,\n",
    "            general_params={\n",
    "                'nested_cv': False,\n",
    "                'use_algos': [['xgb']]\n",
    "            },\n",
    "            reader_params={\n",
    "                'npartitions': 2\n",
    "            },\n",
    "            text_params={\n",
    "                'lang': 'ru',\n",
    "                'verbose': False,\n",
    "                'use_stem': False,\n",
    "            },\n",
    "            tfidf_params={\n",
    "                'n_components': n_components,\n",
    "                'n_oversample': n_oversample,\n",
    "                'tfidf_params': {'ngram_range': ngram}\n",
    "            },\n",
    "            linear_pipeline_params={\n",
    "                'text_features': \"tfidf\"\n",
    "            }\n",
    "            )\n",
    "\n",
    "run_automl(automl, tr_data, te_data)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-22.10",
   "language": "python",
   "name": "rapids-22.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
